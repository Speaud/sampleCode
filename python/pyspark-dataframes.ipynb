{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, min, max, rand, randn\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-139.507999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Decimal('172.816876'),\n",
       " Decimal('-13.879081'),\n",
       " Decimal('-69.892454'),\n",
       " Decimal('4.431388'),\n",
       " Decimal('-128.536729'),\n",
       " Decimal('146.388684'),\n",
       " Decimal('172.841787'),\n",
       " Decimal('151.936922'),\n",
       " Decimal('-127.637413'),\n",
       " Decimal('151.866261')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fake = Faker()\n",
    "tags = fake.coordinate(center=None, radius=0.001)\n",
    "print(tags)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return fake.coordinate(center=None, radius=0.001)\n",
    "\n",
    "list(map(lambda x: f(x), list(range(10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Session (SQL Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('demo') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt ways to create a spark dataframe\n",
    "#alt_way_a = spark.createDataFrame([\"10\",\"11\",\"13\"], \"string\").toDF(\"age\")\n",
    "#alt_way_b = spark.createDataFrame([(\"10\", ), (\"11\", ), (\"13\",  )], [\"age\"])\n",
    "#from pyspark.sql.types import StringType\n",
    "#spark.createDataFrame([\"10\",\"11\",\"13\"], StringType()).toDF(\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "        .range(0, 10) \\\n",
    "        .withColumn('uniform', rand(seed=13)) \\\n",
    "        .withColumn('normal', randn(seed=31)) \\\n",
    "        .withColumn('normal', randn(seed=31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- uniform: double (nullable = true)\n",
      " |-- normal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_row = spark.createDataFrame([(10, 1, 1)], ['id', 'uniform', 'normal'])\n",
    "df = df.union(new_row)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- uniform: double (nullable = true)\n",
      " |-- normal: double (nullable = true)\n",
      " |-- apd: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('apd', rand(seed=11))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------+\n",
      "|      avg(uniform)|       min(uniform)|max(uniform)|\n",
      "+------------------+-------------------+------------+\n",
      "|0.4803029184230118|0.06415762022697513|         1.0|\n",
      "+------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([mean('uniform'), min('uniform'), max('uniform')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06948283612954509"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample covariance of two columns\n",
    "df.stat.cov('uniform', 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events\n",
    "e_cols = [\"EVENTID\",\"EVENTTIMESTAMP\",\"EVENTTYPE\",\"PROVIDERID\",\"PATIENTID\",\"TASKID\",\"ENCOUNTERID\",\"WORKSTATIONID\",\"PROCESS_ID\",\"EVENTACTION\"]\n",
    "e_rows = [\n",
    "    (\"5636059882.643001\",\"2019-08-07 00:11:22\",\"View\",\"14173\",\"Z854894\",\"17008\",\"30041094735\",\"10003000\",\"prd-32835774\",\"QUERY\"),\n",
    "    (\"5636059882.643001\",\"2019-08-07 00:11:22\",\"View\",\"14173\",\"Z854894\",\"17008\",\"30041094735\",\"99001647\",\"prd-32835774\",\"QUERY\"),\n",
    "    (\"5636059882.643001\",\"2019-08-07 00:11:22\",\"View\",\"14173\",\"Z854894\",\"17008\",\"30041094735\",\"99001647\",\"prd-32835774\",\"QUERY\"),\n",
    "    (\"5636059882.643001\",\"2019-08-07 00:11:22\",\"View\",\"14173\",\"Z854894\",\"17008\",\"30041094735\",\"20000073\",\"prd-32835774\",\"QUERY\"),\n",
    "    (\"5636059882.643001\",\"2019-08-07 00:11:22\",\"View\",\"14173\",\"Z854894\",\"17008\",\"30041094735\",\"\",\"prd-32835774\",\"QUERY\")\n",
    "]\n",
    "\n",
    "# workstations\n",
    "w_cols = [\"WORKSTATIONID\",\"WORKSTATIONAREA\",\"EXTERNAL_NAME\",\"DEPARTMENT_ID\",\"NETWORK\",\"COMPANY\"]\n",
    "w_rows = [\n",
    "    (\"10003000\",\"BDW ORTHO/NEURO\",\"Brandywine Hospital Ortho/Neuro\",\"107001009\",\"Brandywine\",\"Brandywine Hospital\"),\n",
    "    (\"20000073\",\"FAMILY FLOURTOWN THMG\",\"Springfield Medical Associates\",\"102409001\",\"Chestnut Hill\",\"CHN - Chestnut Hill Clinic\"),\n",
    "    (\"00195B2FF60701080B160C03020E1809\",\"EMH EMERGENCY\",\"EMH Emergency Department\",\"10101100\",\"\",\"\"),\n",
    "    (\"40001111\",\"PHX 2AR\",\"Rehabilitation at Phoenixville Hospital\",\"107601028\",\"Phoenixville\",\"Phoenixville Hospital\"),\n",
    "    (\"50001297\",\"PTN MAIN OR\",\"Pottstown Main OR\",\"107801016\",\"Pottstown\",\"Pottstown Hospital\"),\n",
    "    (\"50001303\",\"PTN EMERGENCY\",\"Pottstown Hospital Emergency Department\",\"107801001\",\"Pottstown\",\"Pottstown Hospital\"),\n",
    "    (\"50001310\",\"INTL MED BOYER THMG\",\"Berks Medical Associates\",\"103201001\",\"Pottstown\",\"PTS - Pottstown Clinic Co\")\n",
    "]\n",
    "\n",
    "e_df = spark.createDataFrame(e_rows, e_cols)\n",
    "w_df = spark.createDataFrame(w_rows, w_cols)\n",
    "\n",
    "lte_df = w_df.select(w_df['WORKSTATIONID'], w_df['NETWORK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EVENTID: string (nullable = true)\n",
      " |-- EVENTTIMESTAMP: string (nullable = true)\n",
      " |-- EVENTTYPE: string (nullable = true)\n",
      " |-- PROVIDERID: string (nullable = true)\n",
      " |-- PATIENTID: string (nullable = true)\n",
      " |-- TASKID: string (nullable = true)\n",
      " |-- ENCOUNTERID: string (nullable = true)\n",
      " |-- WORKSTATIONID: string (nullable = true)\n",
      " |-- PROCESS_ID: string (nullable = true)\n",
      " |-- EVENTACTION: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+---------+----------+---------+------+-----------+-------------+------------+-----------+\n",
      "|          EVENTID|     EVENTTIMESTAMP|EVENTTYPE|PROVIDERID|PATIENTID|TASKID|ENCOUNTERID|WORKSTATIONID|  PROCESS_ID|EVENTACTION|\n",
      "+-----------------+-------------------+---------+----------+---------+------+-----------+-------------+------------+-----------+\n",
      "|5636059882.643001|2019-08-07 00:11:22|     View|     14173|  Z854894| 17008|30041094735|     10003000|prd-32835774|      QUERY|\n",
      "|5636059882.643001|2019-08-07 00:11:22|     View|     14173|  Z854894| 17008|30041094735|     99001647|prd-32835774|      QUERY|\n",
      "|5636059882.643001|2019-08-07 00:11:22|     View|     14173|  Z854894| 17008|30041094735|     99001647|prd-32835774|      QUERY|\n",
      "|5636059882.643001|2019-08-07 00:11:22|     View|     14173|  Z854894| 17008|30041094735|     20000073|prd-32835774|      QUERY|\n",
      "|5636059882.643001|2019-08-07 00:11:22|     View|     14173|  Z854894| 17008|30041094735|             |prd-32835774|      QUERY|\n",
      "+-----------------+-------------------+---------+----------+---------+------+-----------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_df.printSchema()\n",
    "e_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = w_df.filter()select(w_df['WORKSTATIONID'], w_df['NETWORK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|       WORKSTATIONID|      NETWORK|\n",
      "+--------------------+-------------+\n",
      "|            10003000|   Brandywine|\n",
      "|            20000073|Chestnut Hill|\n",
      "|00195B2FF60701080...|             |\n",
      "|            40001111| Phoenixville|\n",
      "|            50001297|    Pottstown|\n",
      "|            50001303|    Pottstown|\n",
      "|            50001310|    Pottstown|\n",
      "|                   4|          ASD|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newRow = spark.createDataFrame([(\"4\", \"ASD\")], ['WORKSTATIONID', 'NETWORK'])\n",
    "appended = lte_df.union(newRow)\n",
    "appended.show()\n",
    "\n",
    "# .filter(df.age > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df.printSchema()\n",
    "w_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = e_df.join(lte_df, \"WORKSTATIONID\", \"right_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-------------------+---------+----------+---------+------+-----------+------------+-----------+-------------+\n",
      "|WORKSTATIONID|          EVENTID|     EVENTTIMESTAMP|EVENTTYPE|PROVIDERID|PATIENTID|TASKID|ENCOUNTERID|  PROCESS_ID|EVENTACTION|      NETWORK|\n",
      "+-------------+-----------------+-------------------+---------+----------+---------+------+-----------+------------+-----------+-------------+\n",
      "|     20000073|5636059882.643001|2019-08-07 00:11:22|     View|     14173|  Z854894| 17008|30041094735|prd-32835774|      QUERY|Chestnut Hill|\n",
      "|     10003000|5636059882.643001|2019-08-07 00:11:22|     View|     14173|  Z854894| 17008|30041094735|prd-32835774|      QUERY|   Brandywine|\n",
      "+-------------+-----------------+-------------------+---------+----------+---------+------+-----------+------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['EVENTID'] != '').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "a = df.groupBy('NETWORK').count()\n",
    "\n",
    "def pctVal(x):\n",
    "    #return round(a['count']/df.count(),2)\n",
    "    b = round(x/df.count(),2)\n",
    "    return b\n",
    "    \n",
    "a.withColumn('pct', round(a['count']/df.count(), 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(v):\n",
    "    print(a[v])\n",
    "    return v\n",
    "\n",
    "a.select('NETWORK', func('count')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
